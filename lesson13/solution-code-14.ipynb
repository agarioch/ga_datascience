{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Demo\n",
    "\n",
    "If you haven't installed spacy yet, use:\n",
    "```\n",
    "conda install spacy\n",
    "python -m spacy.en.download\n",
    "```\n",
    "This downloads about 500 MB of data.\n",
    "\n",
    "Another popular package, `nltk`, can be installed as follows (you can skip this for now):\n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "python -m nltk.downloader all\n",
    "```\n",
    "\n",
    "This also downloads a lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load StumbleUpon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"stumbleupon.tsv\", sep='\\t', encoding=\"utf-8\")\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x10cfcf630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_toolkit = spacy.load(\"en\")\n",
    "nlp_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: IBM\n",
      "\t Phrase type: nsubj\n",
      "\t Father Node: sees\n",
      "\t Is the word a known entity type? ORG\n",
      "\t Lemma: ibm\n",
      "\t Parent of this word: see\n",
      "Word: sees\n",
      "\t Phrase type: ROOT\n",
      "\t Father Node: sees\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: see\n",
      "\t Parent of this word: see\n",
      "Word: holographic\n",
      "\t Phrase type: amod\n",
      "\t Father Node: calls\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: holographic\n",
      "\t Parent of this word: call\n",
      "Word: calls\n",
      "\t Phrase type: dobj\n",
      "\t Father Node: sees\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: call\n",
      "\t Parent of this word: see\n",
      "Word: ,\n",
      "\t Phrase type: punct\n",
      "\t Father Node: calls\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: ,\n",
      "\t Parent of this word: call\n",
      "Word: air\n",
      "\t Phrase type: compound\n",
      "\t Father Node: breathing\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: air\n",
      "\t Parent of this word: breathing\n",
      "Word: breathing\n",
      "\t Phrase type: compound\n",
      "\t Father Node: batteries\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: breathing\n",
      "\t Parent of this word: battery\n",
      "Word: batteries\n",
      "\t Phrase type: conj\n",
      "\t Father Node: calls\n",
      "\t Is the word a known entity type? No\n",
      "\t Lemma: battery\n",
      "\t Parent of this word: call\n"
     ]
    }
   ],
   "source": [
    "title = u\"IBM sees holographic calls, air breathing batteries\"\n",
    "parsed = nlp_toolkit(title)\n",
    "\n",
    "for (i, word) in enumerate(parsed): \n",
    "    print( \"Word: {}\".format(word))\n",
    "    print( \"\\t Phrase type: {}\".format(word.dep_))\n",
    "    print( \"\\t Father Node: {}\".format(word.head.text))\n",
    "    print( \"\\t Is the word a known entity type? {}\".format(\n",
    "        word.ent_type_  if word.ent_type_ else \"No\"))\n",
    "    print( \"\\t Lemma: {}\".format(word.lemma_))\n",
    "    print( \"\\t Parent of this word: {}\".format(word.head.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sees                \n",
      "  _______________|_____              \n",
      " |                   calls          \n",
      " |        _____________|_______      \n",
      " |       |             |   batteries\n",
      " |       |             |       |     \n",
      " |       |             |   breathing\n",
      " |       |             |       |     \n",
      "IBM holographic        ,      air   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEMO code to display the dependency tree.\n",
    "from nltk import Tree\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in parsed.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Page Titles\n",
    "\n",
    "Let's see if we can find organizations in our page titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Fully Electronic Futuristic Starting Gun That Eliminates Advantages in Races the fully electronic, futuristic starting gun that eliminates advantages in races the fully electronic, futuristic starting gun that eliminates advantages in races</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Foolproof Tips for Better Sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fashion lane American Wild Child</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Business Financial News Breaking US International News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                   title\n",
       "0   IBM Sees Holographic Calls Air Breathing Batteries ibm sees holographic calls, air-breathing batteries                                                                                                                                              \n",
       "1   The Fully Electronic Futuristic Starting Gun That Eliminates Advantages in Races the fully electronic, futuristic starting gun that eliminates advantages in races the fully electronic, futuristic starting gun that eliminates advantages in races\n",
       "3   10 Foolproof Tips for Better Sleep                                                                                                                                                                                                                  \n",
       "6   fashion lane American Wild Child                                                                                                                                                                                                                    \n",
       "10  Business Financial News Breaking US International News                                                                                                                                                                                              "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def references_organization(title):\n",
    "    parsed = nlp_toolkit(title)\n",
    "    return any([word.ent_type_ == 'ORG' for word in parsed])\n",
    "\n",
    "data['references_organization'] = data['title'].fillna(u'').map(references_organization)\n",
    "\n",
    "# Take a look\n",
    "data[data['references_organization']][['title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    True \n",
       "1    True \n",
       "2    False\n",
       "3    True \n",
       "4    False\n",
       "Name: references_organization, dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['references_organization'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Lets write a function to identify titles that mention an organization (ORG) and a person (PERSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Genevieve Morton Swimsuit by Tyler Rose Swimwear 2011 Sports Illustrated Swimsuit Photo Gallery genevieve morton - model - 2011 sports illustrated swimsuit edition - si.com genevieve morton on si swimsuit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Alyssa Miller Swimsuit by Charlie by Matthew Zink 2011 Sports Illustrated Swimsuit Photo Gallery alyssa miller - maui action - 2011 sports illustrated swimsuit edition - si.com alyssa miller on si swimsuit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>4 Surprising Foods to Cook on the Grill Whisked Foodie 4 surprising foods to cook on the grill | whisked foodie | whisk up something delicious.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Heidi s Favorite Snacks Heidi Klum on AOL heidi's favorite snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Chicken and Spinach Casserole Martha Stewart Recipes chicken and spinach casserole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                             title\n",
       "29   Genevieve Morton Swimsuit by Tyler Rose Swimwear 2011 Sports Illustrated Swimsuit Photo Gallery genevieve morton - model - 2011 sports illustrated swimsuit edition - si.com genevieve morton on si swimsuit \n",
       "44   Alyssa Miller Swimsuit by Charlie by Matthew Zink 2011 Sports Illustrated Swimsuit Photo Gallery alyssa miller - maui action - 2011 sports illustrated swimsuit edition - si.com alyssa miller on si swimsuit\n",
       "89   4 Surprising Foods to Cook on the Grill Whisked Foodie 4 surprising foods to cook on the grill | whisked foodie | whisk up something delicious.                                                              \n",
       "91   Heidi s Favorite Snacks Heidi Klum on AOL heidi's favorite snacks                                                                                                                                            \n",
       "105  Chicken and Spinach Casserole Martha Stewart Recipes chicken and spinach casserole                                                                                                                           "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Exercise solution\n",
    "def references_org_person(title):\n",
    "    parsed = nlp_toolkit(title)\n",
    "    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n",
    "    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n",
    "    return contains_org and contains_person\n",
    "\n",
    "data['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\n",
    "\n",
    "# Take a look\n",
    "data[data['references_org_person']][['title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting \"Greenness\" Of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes from [stumbleupon](https://www.stumbleupon.com/), a web page recommender.  \n",
    "\n",
    "A description of the columns is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FieldName|Type|Description\n",
    "---------|----|-----------\n",
    "url|string|Url of the webpage to be classified\n",
    "title|string|Title of the article\n",
    "body|string|Body text of article\n",
    "urlid|integer| StumbleUpon's unique identifier for each url\n",
    "boilerplate|json|Boilerplate text\n",
    "alchemy_category|string|Alchemy category (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "alchemy_category_score|double|Alchemy category score (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "avglinksize| double|Average number of words in each link\n",
    "commonlinkratio_1|double|# of links sharing at least 1 word with 1 other links / # of links\n",
    "commonlinkratio_2|double|# of links sharing at least 1 word with 2 other links / # of links\n",
    "commonlinkratio_3|double|# of links sharing at least 1 word with 3 other links / # of links\n",
    "commonlinkratio_4|double|# of links sharing at least 1 word with 4 other links / # of links\n",
    "compression_ratio|double|Compression achieved on this page via gzip (measure of redundancy)\n",
    "embed_ratio|double|Count of number of <embed> usage\n",
    "frameBased|integer (0 or 1)|A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "frameTagRatio|double|Ratio of iframe markups over total number of markups\n",
    "hasDomainLink|integer (0 or 1)|True (1) if it contains an <a> with an url with domain\n",
    "html_ratio|double|Ratio of tags vs text in the page\n",
    "image_ratio|double|Ratio of <img> tags vs text in the page\n",
    "is_news|integer (0 or 1) | True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "lengthyLinkDomain| integer (0 or 1)|True (1) if at least 3 <a> 's text contains more than 30 alphanumeric characters\n",
    "linkwordscore|double|Percentage of words on the page that are in hyperlink's text\n",
    "news_front_page| integer (0 or 1)|True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "non_markup_alphanum_characters|integer| Page's text's number of alphanumeric characters\n",
    "numberOfLinks|integer Number of <a>|markups\n",
    "numwords_in_url| double|Number of words in url\n",
    "parametrizedLinkRatio|double|A link is parametrized if it's url contains parameters or has an attached onClick event\n",
    "spelling_errors_ratio|double|Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "label|integer (0 or 1)|User-determined label. Either evergreen (1) or non-evergreen (0); available for train.tsv only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Use of the Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = data['title'].fillna('')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words='english',\n",
    "                             binary=True)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Build a random forest model to predict evergreeness of a website using the title features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy [0.7270884  0.73752535 0.73782468], Average Accuracy 0.7341461441883778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 20)\n",
    "    \n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles).toarray()\n",
    "y = data['label']\n",
    "\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy')\n",
    "print('CV Accuracy {}, Average Accuracy {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build a random forest model to predict evergreeness of a website using the title features and quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy [0.72506083 0.73306288 0.72199675], Average Accuracy 0.7267068202739684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>html_ratio</td>\n",
       "      <td>0.156436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>image_ratio</td>\n",
       "      <td>0.094733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>recipe</td>\n",
       "      <td>0.037826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>recipes</td>\n",
       "      <td>0.021396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>chocolate</td>\n",
       "      <td>0.012656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Features  Importance Score\n",
       "1000  html_ratio   0.156436        \n",
       "1001  image_ratio  0.094733        \n",
       "715   recipe       0.037826        \n",
       "721   recipes      0.021396        \n",
       "192   chocolate    0.012656        "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X_text_features = vectorizer.transform(titles)\n",
    "\n",
    "# Identify the features you want from the original dataset\n",
    "other_features_columns = ['html_ratio', 'image_ratio']\n",
    "other_features = data[other_features_columns]\n",
    "\n",
    "# Stack them horizontally together\n",
    "# This takes all of the word/n-gram columns and appends on two more columns for `html_ratio` and `image_ratio`\n",
    "from scipy.sparse import hstack\n",
    "X = hstack((X_text_features, other_features)).toarray()\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy')\n",
    "print('CV Accuracy {}, Average Accuracy {}'.format(scores, scores.mean()))\n",
    "\n",
    "# What features of these are most important?\n",
    "model.fit(X, y)\n",
    "\n",
    "all_feature_names = vectorizer.get_feature_names() + other_features_columns\n",
    "feature_importances = pd.DataFrame({'Features' : all_feature_names, 'Importance Score': model.feature_importances_})\n",
    "feature_importances.sort_values('Importance Score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Build a random forest model to predict evergreeness of a website using the body features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy [0.77128954 0.78052738 0.76948052], Average Accuracy 0.7737658135201849\n"
     ]
    }
   ],
   "source": [
    "body_text = data['body'].fillna('')\n",
    "\n",
    "# Use `fit` to learn the vocabulary\n",
    "vectorizer.fit(body_text)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(body_text).toarray()\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy')\n",
    "print('CV Accuracy {}, Average Accuracy {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Use `TfIdfVectorizer` instead of `CountVectorizer` - is this an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy [0.77696675 0.78864097 0.78530844], Average Accuracy 0.7836387209863136\n"
     ]
    }
   ],
   "source": [
    "titles = data['title'].fillna('')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 1000, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words='english')\n",
    "\n",
    "\n",
    "# Use `fit` to learn the vocabulary\n",
    "vectorizer.fit(body_text)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(body_text).toarray()\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy')\n",
    "print('CV Accuracy {}, Average Accuracy {}'.format(scores, scores.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
