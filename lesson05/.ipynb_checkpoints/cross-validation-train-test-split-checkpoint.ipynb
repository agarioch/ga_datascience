{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Cross validation and train-test splits\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- **Describe** test/train/split and cross-validation\n",
    "- **Explain** why we want to use these validation techniques and how they differ\n",
    "- **Split** data into testing and training sets using both test/train/split and cross validation \n",
    "- **Apply** both techniques to score a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Overfitting and underfitting](#overfitting-underfitting)\n",
    "- [Train-test split](#train-test-split)\n",
    "- [K-Fold cross-validation](#cross-val-k-fold)\n",
    "- [Train-test-split demonstration](#demo)\n",
    "    - [Plot a heatmap](#heatmap)\n",
    "    - [Select a single predictor for SLR](#single-predictor)\n",
    "    - [Split data into training and testing](#sklearn-tts)\n",
    "    - [Fit a linear regression on the training data](#fit-on-train)\n",
    "- [K-Fold cross-validation demonstration](#cv-demo)\n",
    "- [Review: negative $R^2$ values](#neg-r2)\n",
    "- [Hold-out sets](#hold-out)\n",
    "- [Conclusion](#conclusion)\n",
    "- [Additional resources](#additional-resources)\n",
    "- [Summary of cross validation procedure](#summary)\n",
    "- [Code for reference](#code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overfitting-underfitting'></a>\n",
    "\n",
    "## Overfitting and underfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization Problem in Regression\n",
    "\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/02/underfitting-overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://tomrobertshaw.net/img/2015/12/overfitting.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is wrong with the first model?**\n",
    "- The underfit model falls short of capturing the complexity of the \"true model\" of the data.\n",
    "\n",
    "**What is wrong with the third model?**\n",
    "- The overfit model is too complex and is modeling random noise in the data.\n",
    "\n",
    "**Middle model is a good compromise.**\n",
    "- It approximates the complexity of the true model and does not model random noise in our sample as true relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://image.slidesharecdn.com/nncollovcapaldo2013-131220052427-phpapp01/95/machine-learning-introduction-to-neural-networks-12-638.jpg?cb=1393073301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train-test-split\"></a>\n",
    "## Train-test split and model validation\n",
    "\n",
    "---\n",
    "\n",
    "So far we've focused on fitting the best model to our data. But is this the best model for our sample data or the best model overall? How do we know?\n",
    "\n",
    "In practice, we need to validate our model's ability to generalize to new data. One very popuplar method for performing model validation is by splitting our data into subsets: data that we *train* our model on, and data that we *test* our model on.\n",
    "\n",
    "The most basic type of \"hold-out\" validation is called **train-test split**. We split our data into two pieces:\n",
    "\n",
    "> **\"Training set\":** the subset of the data that we fit our model on.\n",
    "\n",
    "> **\"Testing set\":** the subset of the data that we evaluate the quality of our predictions on.\n",
    "\n",
    "\n",
    "**Test/train split benefits:**\n",
    "- Testing data can be a proxy for \"future\" data; for prediction-oriented models it is critical to make sure a model performing well on current data will likely perform well on future data.\n",
    "- Can help diagnose and avoid overfitting via model tuning.\n",
    "- Improve the quality of our predictions.\n",
    "\n",
    "**Using train-test split it can happen that the train and test sets are not really representative of the whole data distribution.** For example, if you are not careful it is easy to take a non-random split. \n",
    "\n",
    "Suppose we have salary data on technical professionals that is composed 80% of data from London and 20% elsewhere and is sorted by county. If we split our data into 80% training data and 20% testing data we might inadvertantly select all the London data to train and all the non-London data to test. In this case we have still overfit our data set because we did not sufficiently randomise the data. A method to reduce this kind of bias is to use cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross-val-k-fold'></a>\n",
    "\n",
    "## K-Fold cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "K-Fold cross-validation takes the idea of a single train-test split and expands this to *multiple tests* across different train-test splits of your data.\n",
    "\n",
    "For example, if you determine your training set will contain 80% of the data and your testing set will contain the other 20%, you could have 5 different 80/20 splits where the test set in each is a different set of observations. We have:\n",
    "- 5 (K=5) training sets\n",
    "- 5 (K=5) corresponding test sets\n",
    "\n",
    "**K-Fold cross-validation builds K models, one for each train-test pair, and evaluates those models on each respective test-set.**\n",
    "\n",
    "### K-Fold cross-validation visually\n",
    "\n",
    "<img src=\"https://snag.gy/o1lLcw.jpg?convert_to_webp=true\" width=\"500\"a>\n",
    "\n",
    "---\n",
    "\n",
    "Cross-validation helps us understand how a model parameterization may perform in a variety of cases. The K-Fold cross-validation procedure can be described in pseudocode:\n",
    "\n",
    "```\n",
    "set k\n",
    "create k groups of rows in data\n",
    "\n",
    "for group i in k row groups:\n",
    "    test data is data[group i]\n",
    "    train data is data[all groups not i]\n",
    "    \n",
    "    fit model on train data\n",
    "    \n",
    "    score model on test data\n",
    "    \n",
    "evaluate mean of k model scores\n",
    "evaluate variance of k model scores\n",
    "```\n",
    "\n",
    "Odd case #1:\n",
    "> **When K=2**: This is equivalent to doing ***two*** mirror image 50-50 train-test splits.\n",
    "\n",
    "Odd case #2:\n",
    "> **When K=number of rows**: This is known as \"leave one out cross-validation\" or LOOCV. A model is built on all but one row and tested on the single held-out observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "## Train-test split with sklearn demonstration\n",
    "\n",
    "---\n",
    "\n",
    "Let's use sklearn to load everyone's favorite data set: the diabetes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "print(diabetes.keys())\n",
    "\n",
    "#\n",
    "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = diabetes.target\n",
    "\n",
    "# Take a look at the data again\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='heatmap'></a>\n",
    "\n",
    "### Plot a heatmap of the correlation matrix\n",
    "\n",
    "Heatmaps are a great way to visually examine the correlational structure of your predictors. \n",
    "\n",
    "> Keep in mind that pearson correlation between non-dummy-coded categorical variables and other variables are invalid!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heat_map(df):\n",
    "    corrs = df.corr()\n",
    "\n",
    "    # Set the default matplotlib figure size:\n",
    "    fig, ax = plt.subplots(figsize=(11,7))\n",
    "\n",
    "    # Generate a mask for the upper triangle (taken from seaborn example gallery)\n",
    "    mask = np.zeros_like(corrs, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Plot the heatmap with seaborn.\n",
    "    # Assign the matplotlib axis the function returns. This will let us resize the labels.\n",
    "    ax = sns.heatmap(corrs, mask=mask, annot=True, vmin=-1, vmax=1)\n",
    "\n",
    "    # Resize the labels.\n",
    "    ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14, rotation=30)\n",
    "    ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14, rotation=0)\n",
    "\n",
    "    # If you put plt.show() at the bottom, it prevents those useless printouts from matplotlib.\n",
    "    plt.show()\n",
    "\n",
    "df_with_target = df.copy()\n",
    "df_with_target['target'] = y\n",
    "correlation_heat_map(df_with_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single-predictor'></a>\n",
    "\n",
    "### Select a single predictor for a SLR\n",
    "\n",
    "The variable `age` appears to have a minor linear relationship with the target variable.\n",
    "\n",
    "Let's select just `age` out of the data as a single column design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age']]\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Good practice to plot the variable against the target to confirm the relationship visually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('age','target', df_with_target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-tts'></a>\n",
    "\n",
    "### sklearn's `train_test_split` function\n",
    "\n",
    "Train test split using sklearn is easy. Load the `train_test_split` function:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "**Arguments**:\n",
    "- *arrays: Any number of arrays/matrices to split up into training and testing (they should be the same length).\n",
    "- `test_size`: an integer for exact size of the test subset or a float for a percentage\n",
    "- `train_size`: alternatively you can specify the training size\n",
    "- `stratify`: supply a vector to stratify the split (more important in classification tasks)\n",
    "\n",
    "**Perform a 70-30 split of our `X` and `y`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( X_train.shape, y_train.shape)\n",
    "print( X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could always split the data up manually. Here's an [example](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) of manually splitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-on-train'></a>\n",
    "\n",
    "### Fit a linear regression on the training set\n",
    "\n",
    "Using the training `X` and training `y`, we can fit a linear regression with sklearn's `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "slr = LinearRegression()\n",
    "slr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='score-on-test'></a>\n",
    "\n",
    "### Calculate the $R^2$ score on the test data\n",
    "\n",
    "After we have our model constructed on the training set, we can evaluate how well our model performs on data it has no exposure to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare this to the model scored on the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slr.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='neg-r2'></a>\n",
    "## Review: negative $R^2$ values\n",
    "\n",
    "----\n",
    "\n",
    "What does it mean to have a negative $R^2$?\n",
    "\n",
    "A negative $R^2$ only makes sense (and can only be found) when we are evaluating the $R^2$ score on data that the model was not fit on. If $R^2$ is evaluated for a model using the training data, *the minimum $R^2$ must be zero.* \n",
    "\n",
    "However, on a test set the $R^2$ **can** be negative. This means that the model performs so poorly on the testing set that you would have been better off just using the mean of the target from the training set as an estimate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-demo'></a>\n",
    "\n",
    "## K-Fold cross-validation demonstration\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Now let's try out k-fold cross-validation. Again scikit-learn provides useful functions to do the heavy lifting. \n",
    "\n",
    "The function `cross_val_score` returns the $R^2$ for each test set, we can also specify that it should return another metric, like MSE.\n",
    "\n",
    "Alternatively, the function `cross_val_predict` returns the predicted values for each data point when it's in the testing slice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "scores = cross_val_score(slr, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "# Notice that the scores are negative, this is because sklearn normalises scores so a bigger score\n",
    "# is better than a lower score (which is the opposite of how MSE works)\n",
    "print( \"Cross-validated MSE scores:\", -1 * scores )\n",
    "print( \"CV RMSE: \", np.sqrt(np.mean(-1 * scores))) # RMSE error is in the same unit as the original target\n",
    "\n",
    "# Make cross validated predictions on the test sets\n",
    "predictions = cross_val_predict(slr, X, y, cv=5)\n",
    "plt.scatter(y, predictions)\n",
    "\n",
    "# manually calculate the r2\n",
    "mse = metrics.mean_squared_error(y, predictions)\n",
    "print( \"Cross-Predicted MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hold-out'></a>\n",
    "\n",
    "## Hold-out sets\n",
    "\n",
    "---\n",
    "\n",
    "Hold-out sets are a version of train-test split. The concept of having a hold-out set is:\n",
    "1. **Split data into a large train and small test set. This small test set will be the \"hold-out\" set.**\n",
    "2. **For a set of different model parameterizations:**\n",
    "    1. **Set up the model.**\n",
    "    2. **Cross-validate the current model on the training data.**\n",
    "    3. **Save the model performance.**\n",
    "3. **Select the model that performed best using cross-validation on the training data.**\n",
    "4. **Perform a final test of that model on the original \"hold-out\" test set.**\n",
    "\n",
    "> **Note:** The \"hold-out\" method is more conservative, but also requires that you have more data. With smaller datasets it can be infeasable.\n",
    "\n",
    "The graphic below explains the hold-out method visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/Train-Test-Split-CV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- Split data into test and training sets\n",
    "- Performed cross validation scoring\n",
    "- Made cross validation predictions\n",
    "\n",
    "**Benefits of validation:**\n",
    "\n",
    "- Test the model\n",
    "- Avoid overfitting\n",
    "- Assess how well a model generalises to an independet dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Cross-validation Example](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py)\n",
    "- [Plotting Cross-Validated Predictions](http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html)\n",
    "- Examine this [academic paper](http://frostiebek.free.fr/docs/Machine%20Learning/validation-1.pdf) on the underpinnings of the holdout method, LOOVC, and kfolds\n",
    "- The sklearn [documentation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) on cross validation is strong\n",
    "- This [Stanford lesson](https://www.youtube.com/watch?v=_2ij6eaaSl0) on cross validation\n",
    "- This [blog post](http://www.win-vector.com/blog/2015/01/random-testtrain-split-is-not-always-enough/) on why TTS is not always enough\n",
    "- StackExchange [discussion](http://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio) on approximate TTS, validation set sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## Summary of cross validation procedure\n",
    "\n",
    "1. Divide data into training, validation, testing sets\n",
    "2. Select architecture (model type) and training parameters (k)\n",
    "3. Train the model using the training set\n",
    "4. Evaluate the model using the validation set\n",
    "5. Repeat 2-4 selecting different architectures (models) and tuning parameters\n",
    "6. Select the best model\n",
    "7. Assess the model with the final testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
