{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intro to Spark\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/ieVW98.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Identify major data types used within Spark\n",
    "- Basic data munging\n",
    "- Work with SparkUI to evaluate \"jobs\"\n",
    "- Use Spark via python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [What is spark?](#intro)\n",
    "    - [Spark is a distributed framework for parallelized applications](#dist)\n",
    "    - [Spark is an API](#api)\n",
    "    - [Spark is machine learning](#ml)\n",
    "    - [Spark is a framework for building high volume stream processors](#stream)\n",
    "    - [Spark is a SQL interface](#sql)\n",
    "    - [Spark is parallel graph processing](#graph)\n",
    "- [Programming with Spark](#prog)\n",
    "- [Spark UI](#sparkui)\n",
    "- [Using Spark via Pyspark](#using)\n",
    "    - [Spark installation guide](#guide)\n",
    "    - [The spark context](#spark-context)\n",
    "- [RDDs vs DataFrames](#rdd)\n",
    "- [Transformations and Actions](#ta)\n",
    "    - [Common Spark transformations](#common-transformations)\n",
    "    - [Common Spark actions](#common-actions)\n",
    "- [Spark data types](#dtypes)\n",
    "    - [Resilient Distributed Dataset](#rdds)\n",
    "    - [DataFrames](#df)\n",
    "- [Common DataFrame operations and characteristics](#common-df)\n",
    "- [Some basic stats in Spark](#stats)\n",
    "- [Limiting results](#limiting)\n",
    "- [More DataFrame and Series operations](#more-ops)\n",
    "- [Activity: check out another dataset using Spark](#activity)\n",
    "- [Practical tips](#practical-tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## What is Spark?\n",
    "---\n",
    "\n",
    "**How is it different and similar to:**\n",
    "\n",
    "- Hadoop\n",
    "- Map reduce\n",
    "- Random Forrests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this, \"Spark\" you speak of?\n",
    "![](https://snag.gy/4oxeiA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is many things.  \n",
    "\n",
    "- It's a set of tools for developing applications.  \n",
    "- It's parallel processing of applications, in a distributed environment.\n",
    "\n",
    "![](https://snag.gy/c9b1Kx.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dist'></a>\n",
    "### Spark is a distributed computer framework for parallelized applications like _Hadoop_.\n",
    "\n",
    "_Spark can interact with Hadoop's HDFS to access large amounts of data using high volume, distributed I/O._\n",
    "\n",
    "![](https://snag.gy/s8gSlG.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='api'></a>\n",
    "### Spark is an API for handling large data tranformations like _Map Reduce_.\n",
    "\n",
    "_It is a data transformation and selection tool like **Pandas**.  You can develop transformations through an API that allows you to chain operations together in a modular fashion, similar to **Pandas**._\n",
    "\n",
    "![](https://snag.gy/9G4gJO.jpg)\n",
    "\n",
    "> However, Spark delivers the idea of data manipulation through the framework of **transformations** and **actions**.  These transformations and actions are performed in parallel, using many different worker nodes (which may be distributed on multiple machines).\n",
    "\n",
    "> Mapreduce is broken down into two main functions \"map\" and \"reduce\".  Spark on the other hand, operates through a set of operations determined through a **Directed Acyclic Graph** (or DAG for short)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ml'></a>\n",
    "### Spark is machine learning like _Scikit Learn_.\n",
    "\n",
    "![](https://snag.gy/RnuX6h.jpg)\n",
    "\n",
    "_Spark provides an interface to MLib via Scala, Java, Python, and R.  The most common methods are provided such as regression, support vector machines, and random forrests, however, not all evaluation metrics are available in Python yet.  Spark is written in Scala, so features are prioritized to Scala first throughout the Spark ecosystem._\n",
    "\n",
    "> <i class=\"fa fa-question-circe\"></i> Anyone remember specific limitations? (You might not if you haven't done MLlib with Spark yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stream'></a>\n",
    "### Spark is a framework for building high volume stream processors.\n",
    "![](https://snag.gy/RCikuU.jpg)\n",
    "\n",
    "With Spark streaming, it's possible to build a proccess that can respond to data in real-time, using any of Spark's features including Mlib, GraphX, or any kinds of transformations you could do within the Spark context.  The streaming capabilities of Spark core make it possible to prodice real-time applications such as ETL, analytics dashboards, data mining, or large scale aggregations.\n",
    "\n",
    "In short, you can create a \"streaming context\" that listens on a specific port, and tie to to any number of operations that can be programed with spark.\n",
    "\n",
    ">```python\n",
    "># Save this as a file called \"network_streaming.py\"\n",
    ">from pyspark import SparkContext\n",
    ">from pyspark.streaming import StreamingContext\n",
    ">\n",
    "># Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    ">sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    ">ssc = StreamingContext(sc, 1)\n",
    ">\n",
    "># Create a DStream that will connect to hostname:port, like localhost:9999\n",
    ">lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    ">\n",
    "># Split each line into words\n",
    ">words = lines.flatMap(lambda line: line.split(\" \"))\n",
    ">\n",
    "># Count each word in each batch\n",
    ">pairs = words.map(lambda word: (word, 1))\n",
    ">wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    ">\n",
    "># Print the first ten elements of each RDD generated in this DStream to the console\n",
    ">wordCounts.pprint()\n",
    ">\n",
    ">ssc.start()             # Start the computation\n",
    ">ssc.awaitTermination()  # Wait for the computation to terminate\n",
    ">```\n",
    "\n",
    "Then you can have Spark run this stream using the unix utility called **netcat** to route your service once you run spark-submit.\n",
    "\n",
    ">```bash\n",
    ">$ nc -lk 9999\n",
    ">```\n",
    "\n",
    "Running spark-submit launches applications onto your Spark cluster.\n",
    "\n",
    ">```bash\n",
    ">./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sql'></a>\n",
    "### Spark is an SQL interface into dataframes like _Hive_.\n",
    "\n",
    "Spark isn't actactly **Hive**, but it uses components from Hive.  However, you can use Spark dataframes easier to use with temporary SQL views.\n",
    "\n",
    ">```python\n",
    "># Load a dataset as a Spark DataFrame\n",
    ">df = spark.read.csv(\"datasets/somedataset/hamburgers_eaten_per_hour.csv\")\n",
    ">df.createOrReplaceTempView(\"hamburgers\")\n",
    ">```\n",
    "\n",
    "\n",
    "\n",
    "Then viola, you can slice and dice your dataframe as SQL:\n",
    "\n",
    ">```python\n",
    ">spark.sql(\"SELECT * FROM hamburgers\").show()\n",
    ">\n",
    "># +------+---------+\n",
    "># | eaten|     name|\n",
    "># +------+---------+\n",
    "># |null  |     Jeff|\n",
    "># |  30  |   Kiefer|\n",
    "># |  19  |     Hang|\n",
    "># +------+---------+\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='graph'></a>\n",
    "### Spark is also parallel graph processing.\n",
    "\n",
    "![](https://snag.gy/0KvrCw.jpg)\n",
    "\n",
    "Graph processing provides the ability to process data in terms of relationships.  The area of applications where traditional RDBMS systems fail to provide involve calculating relationships between entities that don't have a predetermined depth between sets.  \n",
    "\n",
    "Using graphs, you can represent irregular relationship shapes within data and apply functions recurvively and/or arbirarily on any depth of relationships.\n",
    "\n",
    "Both RDBMS and Graphs can be abused to equate features of the other but each have their strengths and tradeoffs depending on application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prog'></a>\n",
    "## Programming with Spark\n",
    "---\n",
    "\n",
    "Running applications build with Spark has a variety of options:\n",
    "\n",
    "- Pyspark\n",
    "- spark-submit\n",
    "\n",
    "### Pyspark\n",
    "Pyspark is a real-time interpreter for Spark for Python.  You can integrate operations with the Spark Python libraries in real-time.  This is a great way to prototype applications much in the same way we do with Jupyter notebook.  It's also possible to connect Pyspark to Jupyter.\n",
    "\n",
    "### Spark-submit\n",
    "Spark-submit, on the other hand, is a way to run a set of application instructions bundled in a single file. It's possible to write these in Python, Scala, or Java.  Typically, it's convienient to prototype a set of operations that you want performed on a dataset with Pyspark, debug them to a high level of quality, then run them with Spark-submit.\n",
    "\n",
    "> With spark-submit, it's also possible to tune the paramters in which your application will run to a very granular degree including memory, number of total cores, and specific cluster modes to control test vs production deployments.\n",
    ">\n",
    "> [Read more about submitting applications](http://spark.apache.org/docs/latest/submitting-applications.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sparkui'></a>\n",
    "## Spark UI\n",
    "---\n",
    "\n",
    "Anytime a \"spark context\" is created, a cooresponding spark UI is launched.  Whenever you launch a Spark standalone instance, Pyspark, a Spark UI will be created.  Through the web UI, you can monitor how your applications run.  Anything that the Spark context handles, even the one line operations from PySpark can be observed as seperate jobs in Spark UI.\n",
    "\n",
    "### Spark Jobs\n",
    "![](https://snag.gy/JnuSKC.jpg)\n",
    "\n",
    "### Job Metrics\n",
    "![](https://snag.gy/JW2fOb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='using'></a>\n",
    "## Using Spark via Pyspark\n",
    "---\n",
    "\n",
    "If you haven't done so yet, it's nice to be able to install Spark manually so you can get familliar with how to use it locally.  The advantage of this, as apposed to using a virtual machine, is that you should be able to run Spark without the overhead of an additional virtual operating system running on top of your already \"actual\" operating system.\n",
    "\n",
    "<a id='guide'></a>\n",
    "### Spark installation guide\n",
    "> This is a good time to review or install Spark from our guide:\n",
    ">\n",
    ">Make sure you have scala 2.11 (scala on bash, tells you version, you can quit with :quit).\n",
    "http://spark.apache.org/docs/2.1.0/ (installation instructions)\n",
    ">\n",
    ">Run to check you can access the shell: \n",
    ">./bin/spark-shell —master local[2]\n",
    ">\n",
    ">To omit the INFO logs, change conf log4j file from INFO to error and then cp to a file >without the .template\n",
    ">Pip install findspark so we can easily point to our pyspark installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='spark-context'></a>\n",
    "### Pyspark is all about \"sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"sc\"** represents your interface to a running spark cluster manager.  A Spark context is defined as a preconfigured cluster, an application name connected to it.  All **transformations** and **actions** performed by Spark, are handled through the Spark context (aka: **sc**).\n",
    "\n",
    ">**Context for standalone Spark apps**\n",
    ">\n",
    ">If we where to write a standalone app for spark-submit, we would need to create the context manually.\n",
    "\n",
    ">```python\n",
    ">from pyspark import SparkContext\n",
    ">sc = SparkContext(\"local\", \"Simple App\")\n",
    ">```\n",
    "\n",
    "<img src=\"https://snag.gy/iCm4G1.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdd'></a>\n",
    "## RDDs vs DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "The two main types of data objects in Spark are the **Resilient Distributed Dataset** and the **DataFrame**.  Both types represent data in a distributed state.  RDDs store data in a more primitive state such as a list of pairs, integers, floats, or strings.  DataFrames have a rich structure defintion called a **schema** much like a Pandas dataframe.\n",
    "\n",
    "- You use **RDDs** to manage semi-structured data.\n",
    "- You use **DataFrames** to operate on typed series.\n",
    "\n",
    "Both RDDs and DataFrames can contain multiple types of objects.  **DataFrames** are much more constrained because data is represented by a 2 deminsional tabular structure where columns represent variables, and rows as observations.  **RDDs** are much more flexible if your data requires much less structure than a DataFrame while still being able to use _transformation_ methods such as **`map()`** and _action_ methods such as **`reduce()`**.\n",
    "\n",
    ">_Distributed Data in Spark_\n",
    ">![](https://snag.gy/vxVhri.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ta'></a>\n",
    "\n",
    "## _Transformations_ and _Actions_\n",
    "\n",
    "---\n",
    "\n",
    "<a id='common-transformations'></a>\n",
    "### <i class=\"fa fa-cogs\" aria-hidden=\"true\"></i> Common Spark transformations\n",
    "\n",
    "\n",
    "Generally, **transformations** in Spark, modify data as new copies of whichever dataset that is used as input.  An analogy you will find the default docs talking about is _map_ as a **transformation**, and _reduce_ as an **action**.  **Transformations are lazy**, so no computations are performed until an **action** is requested.\n",
    "\n",
    "<style>\n",
    ".no-border:table, th, td {\n",
    "    border: none;\n",
    "    border-left: none;\n",
    "    margin: 10px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table class=\"no-border\" style=\"border: none;\">\n",
    "<tbody class=\"no-border\" style=\"border: none;\"><tr class=\"no-border\" style=\" background: #000000; color: #FFFFFF;\"><th>Transformation</th><th style=\"border: none; text-align: center;\">Meaning</th></tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border: none;\"> <b>map</b>(<i>func</i>) </td>\n",
    "  <td stye=\"border-right: none;\"> Return a new distributed dataset formed by passing each element of the source through a function <i>func</i>. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>filter</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Return a new dataset formed by selecting those elements of the source on which <i>func</i> returns true. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>sample</b>(<i>withReplacement</i>, <i>fraction</i>, <i>seed</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Sample a fraction <i>fraction</i> of the data, with or without replacement, using a given random number generator seed. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>distinct</b>([<i>numTasks</i>])) </td>\n",
    "  <td style=\"border-right: none;\"> Return a new dataset that contains the distinct elements of the source dataset.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>groupByKey</b>([<i>numTasks</i>]) <a name=\"GroupByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs. <br>\n",
    "    <b>Note:</b> If you are grouping in order to perform an aggregation (such as a sum or\n",
    "      average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better\n",
    "      performance.\n",
    "    <br>\n",
    "    <b>Note:</b> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.\n",
    "      You can pass an optional <code>numTasks</code> argument to set a different number of tasks.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>reduceByKey</b>(<i>func</i>, [<i>numTasks</i>]) <a name=\"ReduceByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <i>func</i>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>, <i>combOp</i>, [<i>numTasks</i>]) <a name=\"AggregateByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>sortByKey</b>([<i>ascending</i>], [<i>numTasks</i>]) <a name=\"SortByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>join</b>(<i>otherDataset</i>, [<i>numTasks</i>]) <a name=\"JoinLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "    Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>pipe</b>(<i>command</i>, <i>[envVars]</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the\n",
    "    process's stdin and lines output to its stdout are returned as an RDD of strings. </td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> For more info on **transformations**, check out the [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Spark programming guide on #trasformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations)\n",
    "\n",
    "> A great visual guide for common Spark transformations is available by [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Jeff Thompson @ Databricks](http://training.databricks.com/visualapi.pdf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common-actions'></a>\n",
    "### <i class=\"fa fa-wrench\" aria-hidden=\"true\"></i> Common Spark Actions\n",
    "\n",
    "Spark **actions** are not performed until they are called (like a function) -- common sense right?  Careful, because in _Pandas_ we're accustomed to getting results as a result of most transformations performed.  So, if we have a few operations programmed out, nothing will actually be transformed until we call an **action**.\n",
    "\n",
    "```python\n",
    "# 1. Read text file\n",
    "#########\n",
    "# This line reads a text file from the filesystem.  Each line becomes an element in a list-like object (RDD).\n",
    "# No operations have been done as of yet.  We don't know if the text file actually exists or read any data.\n",
    "#############################################################################################################\n",
    "\n",
    "text_lines = sc.textFile(\"somefile.txt\")\n",
    "\n",
    "# 2. Map lengths of each line\n",
    "#########\n",
    "# Once data is available, we simply count the length of lines (length of string), and create a new RDD \n",
    "# which only has the length of each line as a new object.\n",
    "#############################################################################################################\n",
    "\n",
    "text_line_lengths = text_lines.map(lambda s: len(s))\n",
    "\n",
    "# 3. Reduce to find total sum of lengths\n",
    "#########\n",
    "# Once reduce is called, all prior operations are run.  Meaning, we actually sc.textFile(), \n",
    "# and text_lines.Map(), before finally text_line_lengths.reduce() is ran.\n",
    "#############################################################################################################\n",
    "\n",
    "total_length = text_line_lengths.reduce(lambda a, b: a + b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table\" style=\"border-left: none; border-right: none;\">\n",
    "<tbody style=\"border-left: none;\"><tr style=\"background: black; color: white;\"><th>Action</th><th style=\"text-align: center;\">Meaning</th></tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>reduce</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Aggregate the elements of the dataset using a function <i>func</i> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>collect</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>count</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Return the number of elements in the dataset. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>first</b>() </td>\n",
    "  <td style=\"border-right: none;\"> Return the first element of the dataset (similar to take(1)). </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>take</b>(<i>n</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Return an array with the first <i>n</i> elements of the dataset. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>takeSample</b>(<i>withReplacement</i>, <i>num</i>, [<i>seed</i>]) </td>\n",
    "  <td style=\"border-right: none;\"> Return an array with a random sample of <i>num</i> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>takeOrdered</b>(<i>n</i>, <i>[ordering]</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Return the first <i>n</i> elements of the RDD using either their natural order or a custom comparator. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>saveAsTextFile</b>(<i>path</i>) </td>\n",
    "  <td style=\"border-right: none; border-right: none;\"> Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>countByKey</b>() <a name=\"CountByLink\"></a> </td>\n",
    "  <td style=\"border-right: none;\"> Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key. </td>\n",
    "</tr>\n",
    "<tr style=\"border-left: none; border-right: none;\">\n",
    "  <td style=\"border-left: none;\"> <b>foreach</b>(<i>func</i>) </td>\n",
    "  <td style=\"border-right: none;\"> Run a function <i>func</i> on each element of the dataset. This is usually done for side effects such as updating an <a href=\"#accumulators\">Accumulator</a> or interacting with external storage systems.\n",
    "  <br><b>Note</b>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href=\"#understanding-closures-a-nameclosureslinka\">Understanding closures </a> for more details.</td>\n",
    "</tr>\n",
    "</tbody></table>\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> For more info on **actions**, check out the [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Spark programming guide on #actions](http://spark.apache.org/docs/latest/programming-guide.html#actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i>  How might you approach programming with Spark vs Pandas?\n",
    "\n",
    "_Given that operations are \"lazy\"?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dtypes'></a>\n",
    "## Spark data types\n",
    "---\n",
    "\n",
    "### RDD's\n",
    "\n",
    "It's best to think of RDDs as primitive objects that are distributed.  RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.  The RDD type is the oldest type and has been around the Spark codebase since the version 1.0 days.\n",
    "\n",
    "> Neat trick:  Since RDDs can be Python class types, it's possible to do things like run scikit-learn's `GridSearch` over an existing scikit model and paramter search over a massive amount of permutations in a clustered Spark setup.\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "As part of the \"Tungsten Initiative\", which sought to improve the performance of Spark, DataFrames entered the Spark codebase in version 1.3.  The big difference between RDD's and DataFrames, is that DataFrames introduce the idea of a \"schema\" much like Pandas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rdds'></a>\n",
    "<span style=\"font-size: 25pt; font-weight: light;\"><strong>R</strong>eslient <strong>D</strong>istributed <strong>D</strong>ataset</span>\n",
    "<hr>\n",
    "\n",
    "\n",
    "Everything in Spark revolves around the **RDD**.\n",
    "\n",
    "> \"..a fault-tolerant collection of elements that can be operated on in parallel...\"\n",
    "> _-Spark Documentation_\n",
    "\n",
    "Most datasets that can be loaded externally (csv, HDFS/hadoop, text files, RDBMS/SQL, etc), become an RDD and thus operated on in parallel within a distruted system (Spark!).  Otherwise, you can `sc.parallelize(my_data)` a dataset through a Spark context (aka a driver program as it's sometimes called)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check out this Gridsearch with Spark package on [<i class=\"fa fa-external-link\" aria-hidden=\"true\"></i> Github: spark-sklearn](https://github.com/databricks/spark-sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df'></a>\n",
    "### DataFrames\n",
    "\n",
    "The big plus is that Spark DataFrames serializes its data at a lower level to native Java/Scala, so when it's passed between nodes, it's much more performant, requiring fewer processes to handle computations.  Mainly data can be processed faster when it's optimized to a common format [the schema] that Spark doesn't have to convert to in order to perform tasks on it.\n",
    "\n",
    "Outside of the performance optimizations introduced with a schema-based datastructure, the **DataFrame API** provides a convinient set of selectors for transforming data, much like Pandas.  Lastly, it's possible to create temporary views in which **DataFrames** can be queried with SQL - **SparkSQL**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending the driver its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-aeb12656fc01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"myAppName\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alistair/spark-2.3.0-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/Alistair/spark-2.3.0-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alistair/spark-2.3.0-bin-hadoop2.7/python/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mcallback_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgateway_port\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending the driver its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# In Windows, ensure the Java child processes do not linger after Python has exited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending the driver its port number"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/Users/Alistair/spark-2.3.0-bin-hadoop2.7\")\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(\"local\", appName=\"myAppName\")\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-13a6e23ef639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = spark.read.csv(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mpath\u001b[0m        \u001b[0;34m=\u001b[0m   \u001b[0;34m\"datasets/sentiment_words_simple.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mheader\u001b[0m      \u001b[0;34m=\u001b[0m   \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmode\u001b[0m        \u001b[0;34m=\u001b[0m   \u001b[0;34m\"DROPMALFORMED\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# Poorly formed rows in CSV are dropped rather than erroring entire operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minferSchema\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0;32mTrue\u001b[0m               \u001b[0;31m# Not always perfect but works well in most cases as of 2.1+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    path        =   \"datasets/sentiment_words_simple.csv\",\n",
    "    header      =   True, \n",
    "    mode        =   \"DROPMALFORMED\",   # Poorly formed rows in CSV are dropped rather than erroring entire operation\n",
    "    inferSchema =   True               # Not always perfect but works well in most cases as of 2.1+\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- pos_score: double (nullable = true)\n",
      " |-- neg_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='common-df'></a>\n",
    "## Common DataFrame operations and characteristics\n",
    "---\n",
    "\n",
    "Let's have a look at some familliar and new functions and properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect variable / column space of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos', 'word', 'pos_score', 'neg_score']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTypes\n",
    "Inspect schema programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pos', 'string'),\n",
       " ('word', 'string'),\n",
       " ('pos_score', 'double'),\n",
       " ('neg_score', 'double')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain DataFrame\n",
    "Show details about DataFrame type, schema, and origin of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan csv [pos#10,word#11,pos_score#12,neg_score#13] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/cave/Desktop/dsi/ga/dat21/lesson17/intro_to_spark/datasets/sentimen..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pos:string,word:string,pos_score:double,neg_score:double>\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe\n",
    "Describe will look similar to synonymous Pandas **Describe** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "|summary|   pos|                word|          pos_score|          neg_score|\n",
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "|  count|155287|              155287|             155287|             155287|\n",
      "|   mean|  null| 5.687506389495568E9|0.03865380738372139|0.05048873043068892|\n",
      "| stddev|  null|7.537744261701462E10|0.11118246263109259| 0.1392276319041252|\n",
      "|    min|   adj|               'hood|                0.0|                0.0|\n",
      "|    max|  verb|              zyrian|                1.0|                1.0|\n",
      "+-------+------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>155287.000000</td>\n",
       "      <td>155287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.038654</td>\n",
       "      <td>0.050489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.111182</td>\n",
       "      <td>0.139228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pos_score      neg_score\n",
       "count  155287.000000  155287.000000\n",
       "mean        0.038654       0.050489\n",
       "std         0.111182       0.139228\n",
       "min         0.000000       0.000000\n",
       "25%         0.000000       0.000000\n",
       "50%         0.000000       0.000000\n",
       "75%         0.000000       0.000000\n",
       "max         1.000000       1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check Pandas version here!!!\n",
    "df.toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### printSchema\n",
    "The schema is a very import characteristic of a Spark DataFrame.  It tells us what's possible in terms of transformation.  Also, it's the reason DataFrames are so fast since they are typed to a set number of types that are serialized and optimized in Java/Scala behind the scenes.\n",
    "\n",
    "><i class=\"fa fa-exclamation-triangle\" aria-hidden=\"true\"></i> The \"schema\" that we've been so excited to see is finally here to explore.  Feel free to take a screenshot and show your friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pos: string (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- pos_score: double (nullable = true)\n",
      " |-- neg_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count\n",
    "Count with caveat:  This will return the count of all rows, including _non-NaN_ values.  Pandas will omit these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155287"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stats'></a>\n",
    "## Some basic stats in spark\n",
    "---\n",
    "\n",
    "### Covariance\n",
    "<span style=\"font-size: 20pt;\">\n",
    "$\\operatorname{cov}(X,Y) = \\operatorname{E}{\\big[(X - \\operatorname{E}[X])(Y - \\operatorname{E}[Y])\\big]}$\n",
    "</span>\n",
    "> \"Covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019032076361625806"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cov(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Score\n",
    "<span style=\"font-size: 20pt;\">\n",
    "$\\rho_{X,Y}= \\frac{\\operatorname{cov}(X,Y)}{\\sigma_X \\sigma_Y} $\n",
    "</span>\n",
    "> The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12294884293406051"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr(\"pos_score\", \"neg_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='limiting'></a>\n",
    "## Limiting results\n",
    "\n",
    "---\n",
    "\n",
    "### Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pos='adj', word='.22-caliber', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22-calibre', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22_caliber', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.22_calibre', pos_score=0.0, neg_score=0.0),\n",
       " Row(pos='adj', word='.38-caliber', pos_score=0.0, neg_score=0.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5) # all the data is loaded into an instances memory -- use for small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show limited results\n",
    "\n",
    "With Pandas we're used to the `df.head()` as a first step in exploring a dataset.  With Spark this isn't exactly the same.  You need to use the `df.show()`, operation in order to explore data as a first step.  Where Pandas formats its DataFrame output for display in nice HTML tables with sensible defaults for output, you have have to be a bit more specific about what you're looking at with `show()` when using Spark.\n",
    "\n",
    "\n",
    "> The paramters `truncate` is helpful for truncating attributes for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------+---------+\n",
      "|pos|    word|pos_score|neg_score|\n",
      "+---+--------+---------+---------+\n",
      "|adj|.22-c...|      0.0|      0.0|\n",
      "|adj|.22-c...|      0.0|      0.0|\n",
      "|adj|.22_c...|      0.0|      0.0|\n",
      "|adj|.22_c...|      0.0|      0.0|\n",
      "|adj|.38-c...|      0.0|      0.0|\n",
      "+---+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|\n",
      "|adj|.38_caliber|      0.0|      0.0|\n",
      "|adj|.38_calibre|      0.0|      0.0|\n",
      "|adj|.45-caliber|      0.0|      0.0|\n",
      "|adj|.45-calibre|      0.0|      0.0|\n",
      "|adj|.45_caliber|      0.0|      0.0|\n",
      "|adj|.45_calibre|      0.0|      0.0|\n",
      "|adj|          0|      0.0|      0.5|\n",
      "|adj|          1|      0.0|     0.25|\n",
      "|adj|         10|      0.0|      0.0|\n",
      "|adj|10-membered|      0.0|      0.0|\n",
      "|adj|        100|      0.0|      0.0|\n",
      "|adj|       1000|      0.0|      0.0|\n",
      "|adj|     1000th|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(19).show() \n",
    "#`show()` can also be chained to certain outputs like `limit`.  \n",
    "#`show` by itself is a compound operation for displaying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i> Why should you need to be careful when displaying data in Spark?\n",
    "\n",
    "Hopefully you can see why Pandas is so nice to use for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more-ops'></a>\n",
    "## More DataFrame and Series operations\n",
    "---\n",
    "\n",
    "### Convert from list of Row objects to list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.22-caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.22-calibre'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.22_caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.22_calibre'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.38-caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.38-calibre'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.38_caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.38_calibre'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.45-caliber'},\n",
       " {'neg_score': 0.0, 'pos': 'adj', 'pos_score': 0.0, 'word': '.45-calibre'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.asDict() for row in df.take(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting DataFrame Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting variables with Spark **DataFrames API** works very similliarly to Pandas DataFrames.  When selecting variables in Pandas use use a `list` object, passed to a DataFrame object via `[]` brackets like so:\n",
    "\n",
    ">```df[['col1', 'col2']]```\n",
    "\n",
    "The equivalent in spark is using the `.select()` method which takes flat parameters.\n",
    "\n",
    "> <i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i> DataFrames in Pandas are implemented with an overloaded brackets operator so whenever we reference any Pandas DataFrames object with brackets, it get's passed down to a function that handles the column references (as a list), as the input to a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select all features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pos: string, word: string, pos_score: double, neg_score: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.columns).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select specific features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|       word|pos_score|\n",
      "+-----------+---------+\n",
      "|.22-caliber|      0.0|\n",
      "|.22-calibre|      0.0|\n",
      "|.22_caliber|      0.0|\n",
      "|.22_calibre|      0.0|\n",
      "|.38-caliber|      0.0|\n",
      "|.38-calibre|      0.0|\n",
      "|.38_caliber|      0.0|\n",
      "|.38_calibre|      0.0|\n",
      "|.45-caliber|      0.0|\n",
      "|.45-calibre|      0.0|\n",
      "|.45_caliber|      0.0|\n",
      "|.45_calibre|      0.0|\n",
      "|          0|      0.0|\n",
      "|          1|      0.0|\n",
      "|         10|      0.0|\n",
      "|10-membered|      0.0|\n",
      "|        100|      0.0|\n",
      "|       1000|      0.0|\n",
      "|     1000th|      0.0|\n",
      "|      100th|      0.0|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"word\", \"pos_score\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Operations\n",
    "\n",
    "With Pandas, you can easily create a new series that's the sum of every row in **\"col1\"**, to **\"col2\"** like so:\n",
    "\n",
    "> `df['col1'] + df['col2']`\n",
    "\n",
    "In Spark, we have to do this through the select function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|(pos_score + neg_score)|\n",
      "+-----------------------+\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.0|\n",
      "|                    0.5|\n",
      "|                   0.25|\n",
      "|                    0.0|\n",
      "+-----------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"pos_score\"] + df[\"neg_score\"]).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|(pos_score + 10)|\n",
      "+----------------+\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "|            10.0|\n",
      "+----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"pos_score\"] + 10).show(15) ## We can also do math operations in series as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As an \"alias\"\n",
    "As selections are keyed by conditions, they can become hard to read.  We can use an \"alias\" to abstract any selections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_score|\n",
      "+-----------+\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.0|\n",
      "|        0.5|\n",
      "|       0.25|\n",
      "|        0.0|\n",
      "+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select((df[\"pos_score\"] + df[\"neg_score\"]).alias(\"total_score\")).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new features / variables / columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+----------+\n",
      "|pos|       word|pos_score|neg_score|new_column|\n",
      "+---+-----------+---------+---------+----------+\n",
      "|adj|.22-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.38-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.38_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.38_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.45-caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.45-calibre|      0.0|      0.0|       0.0|\n",
      "|adj|.45_caliber|      0.0|      0.0|       0.0|\n",
      "|adj|.45_calibre|      0.0|      0.0|       0.0|\n",
      "|adj|          0|      0.0|      0.5|       0.5|\n",
      "|adj|          1|      0.0|     0.25|      0.25|\n",
      "|adj|         10|      0.0|      0.0|       0.0|\n",
      "+---+-----------+---------+---------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['pos_score'] + df['neg_score'])\n",
    "df.withColumn(\"new_column\", df['pos_score'] + df['neg_score']).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i class=\"fa fa-question-circle\"></i> Have we changed the original DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data\n",
    "In Pandas we use \"masks\" through dataframe object brackets in order to filter data.\n",
    ">`df[df['feature'] > 0]`\n",
    "\n",
    "In Spark, we use the `filter()` method to select different aspects of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|      a-one|    0.625|      0.0|\n",
      "|adj|   abatable|    0.625|      0.0|\n",
      "|adj|abolishable|    0.625|      0.0|\n",
      "|adj|   absolved|    0.625|      0.0|\n",
      "|adj|        ace|    0.625|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"pos_score\"] > .5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple conditions\n",
    "SAME AS PANDAS!  Thankfully, we don't have to leave our comfort zone with too many oddities here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+---------------+\n",
      "|pos|         word|     pos_score|      neg_score|\n",
      "+---+-------------+--------------+---------------+\n",
      "|adj|    adjustive|         0.625|          0.125|\n",
      "|adj|adventuresome|         0.625|           0.25|\n",
      "|adj|  adventurous|         0.625|           0.25|\n",
      "|adj|  affirmative|0.583333333333|0.0416666666667|\n",
      "|adj|all-important|          0.75|          0.125|\n",
      "+---+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"pos_score\"] > .5) & (df[\"neg_score\"] > 0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+---------------+\n",
      "|pos|         word|     pos_score|      neg_score|\n",
      "+---+-------------+--------------+---------------+\n",
      "|adj|    adjustive|         0.625|          0.125|\n",
      "|adj|adventuresome|         0.625|           0.25|\n",
      "|adj|  adventurous|         0.625|           0.25|\n",
      "|adj|  affirmative|0.583333333333|0.0416666666667|\n",
      "|adj|all-important|          0.75|          0.125|\n",
      "+---+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    ".filter(df[\"pos_score\"] > .5) \\\n",
    ".filter(df[\"neg_score\"] > 0).show(5) # Filters can be chained per line using the \\ newline escape sequence character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter as an expression\n",
    "Pandas has a similar function called \"where\".  However, with Spark `filter`, we can filter by shorthand expressions when referencing column sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|      a-one|    0.625|      0.0|\n",
      "|adj|   abatable|    0.625|      0.0|\n",
      "|adj|abolishable|    0.625|      0.0|\n",
      "|adj|   absolved|    0.625|      0.0|\n",
      "|adj|        ace|    0.625|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"pos_score > .5\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------+---------------+\n",
      "|pos|         word|     pos_score|      neg_score|\n",
      "+---+-------------+--------------+---------------+\n",
      "|adj|    adjustive|         0.625|          0.125|\n",
      "|adj|adventuresome|         0.625|           0.25|\n",
      "|adj|  adventurous|         0.625|           0.25|\n",
      "|adj|  affirmative|0.583333333333|0.0416666666667|\n",
      "|adj|all-important|          0.75|          0.125|\n",
      "+---+-------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "condition = \"\"\"\n",
    "pos_score > .5 AND \n",
    "neg_score > 0 \n",
    "\"\"\"\n",
    "\n",
    "df.filter(condition).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+---------+\n",
      "| pos|      word|pos_score|neg_score|\n",
      "+----+----------+---------+---------+\n",
      "| adj| cheapjack|      0.0|      1.0|\n",
      "| adj| henpecked|      0.0|      1.0|\n",
      "| adj|lamentable|      0.0|      1.0|\n",
      "|noun| angriness|      0.0|      1.0|\n",
      "|noun|blackguard|      0.0|      1.0|\n",
      "+----+----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.neg_score.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|  excellent|      1.0|      0.0|\n",
      "|adj| top-flight|      1.0|      0.0|\n",
      "|adj|fantabulous|      1.0|      0.0|\n",
      "|adj|first-class|      1.0|      0.0|\n",
      "|adj|  homologic|      1.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.pos_score.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------+---------+\n",
      "|pos|         word|pos_score|neg_score|\n",
      "+---+-------------+---------+---------+\n",
      "|adj|        awing|    0.875|    0.125|\n",
      "|adj|          fab|    0.875|    0.125|\n",
      "|adj|straightarrow|    0.875|    0.125|\n",
      "|adj|    gladdened|    0.875|    0.125|\n",
      "|adj|      awesome|    0.875|    0.125|\n",
      "+---+-------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"pos_score > .5 AND pos_score < 1.0 AND neg_score > 0\") \\\n",
    ".sort(df.pos_score.desc(), df.neg_score.desc()) \\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+--------------+--------------+\n",
      "| pos|        word|     pos_score|     neg_score|\n",
      "+----+------------+--------------+--------------+\n",
      "| adj|   bona_fide|        0.5125|           0.3|\n",
      "| adj|   authentic|        0.5125|           0.3|\n",
      "| adj|      hearty|         0.525|          0.05|\n",
      "| adj|prophylactic|0.527666666667|0.222333333333|\n",
      "|noun|        joke|       0.53125|        0.1875|\n",
      "+----+------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"pos_score > .5 AND pos_score < 1.0 AND neg_score > 0\") \\\n",
    ".sort(df.pos_score.asc(), df.neg_score.desc()) \\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AS SQL!?\n",
    "\n",
    "\n",
    "Working with DataFrames as SQL is as easy as creating a **temporary view**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+---------+\n",
      "|pos|       word|pos_score|neg_score|\n",
      "+---+-----------+---------+---------+\n",
      "|adj|.22-caliber|      0.0|      0.0|\n",
      "|adj|.22-calibre|      0.0|      0.0|\n",
      "|adj|.22_caliber|      0.0|      0.0|\n",
      "|adj|.22_calibre|      0.0|      0.0|\n",
      "|adj|.38-caliber|      0.0|      0.0|\n",
      "+---+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = spark.sql(\"SELECT * FROM sentiment LIMIT 100\")\n",
    "sentiment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x113566e80>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQsgE8v8BajtPTCzBRo4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11354cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sentiment.toPandas()['neg_score'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Views select DataFrames\n",
    "So the same transformations can be applied to DataFrames as we just learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------------+---------+-------------------+\n",
      "|summary| pos|              word|pos_score|          neg_score|\n",
      "+-------+----+------------------+---------+-------------------+\n",
      "|  count| 100|               100|      100|                100|\n",
      "|   mean|null|  96.9090909090909|      0.0|            0.02625|\n",
      "| stddev|null|154.37463012769066|      0.0|0.09282158132458183|\n",
      "|    min| adj|       .22-caliber|      0.0|                0.0|\n",
      "|    max| adj|              29th|      0.0|              0.625|\n",
      "+-------+----+------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activity'></a>\n",
    "## Activity:  Check out another dataset using Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load up the \"Pokemon\" basic Pokedex dataset\n",
    "First try without infering the schema and without the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|summary|  _c0|  _c1|  _c2|  _c3|  _c4|  _c5|  _c6|  _c7|  _c8|  _c9|\n",
      "+-------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  count|  801|  801|  801|  801|  801|  801|  801|  801|  801|  801|\n",
      "|   mean|36...| null| null|43...|69...|79...|73...|72.82|71...|68...|\n",
      "|  st...|20...| null| null|11...|25...|32...|31...|32...|27...|29...|\n",
      "|    min|  001|Ab...|  Bug|  180|    1|   10|   10|   10|  100|   10|\n",
      "|    max|Po...|Zy...|Wa...|Total|   HP|At...|De...|Sp...|Sp...|Speed|\n",
      "+-------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./datasets/pokedex_basic.csv\", inferSchema=False, header=False)\n",
    "df.describe().show(truncate=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check out the dataset with infer schema paramter but without header.\n",
    "How does it work with / without?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Create a tempory view with the Pokedex DataFrame called \"pokemon\"\n",
    "Then \n",
    "```sql SELECT * FROM pokemon LIMIT 10```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a Which is the strongest Pokemon by `Type`?\n",
    "Using Spark DataFrame operations.  Research Sparks \"grouping\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b Which is the strongest Pokemon by Type?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a Which Pokemon has the best combined Attack and Defence?\n",
    "Using Spark DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.b Which Pokemon has the best combined Attack and Defence?\n",
    "Using the Spark SQL temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create a new feature called \"Pokevalue\" that is the combined Attack, Defence and scaled by .2 of the Pokemon HP.\n",
    "\n",
    "Use any means necessary to solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical-tips'></a>\n",
    "# <i class=\"fa fa-thumbs-up\" aria-hidden=\"true\"></i> Practical tips\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from enormous datasets\n",
    "Undoubtably, you may have the need to examine a larger dataset.  A common operation is to take a sample.  To approximate the characteristics of your global distribution, you should try to adjust the size that best matches the metrics of central tendency or consider doing a power analysis to determine sample sizing.\n",
    "\n",
    "> Size of your sample generally depends on your application be it A/B testing, EDA, Machine Learning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled: 160, Global: 801\n"
     ]
    }
   ],
   "source": [
    "sampled = df.sample(\n",
    "    withReplacement = False, # Unique samples\n",
    "    fraction        = .2,    # Approximate size of sample\n",
    "    seed            = 42     # Similar to scikit's randomState paramter.  Ideal for sharing results.\n",
    ")\n",
    "\n",
    "print (\"Sampled: %d, Global: %d\" % (sampled.count(), df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
